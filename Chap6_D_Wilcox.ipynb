{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "## Daniel Wilcox: 19147414\n",
    "\n",
    "This example problem can be found within chapter 7 of the \"Hands-on Machine Learning with Scikit-Learn and TensorFlow\" by Aurélien Géron. \n",
    "\n",
    "This project will be investigating the theory behind Decision Trees and how to implament them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "from scipy.stats import mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "### 7. Train and fine-tune a Decision Tree for the moons dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Generate a moons dataset using __make_moons(n_samples=10000, noise=0.4)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Data:\n",
    "X_moon, y_moon = make_moons(n_samples=10000, noise=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Split it into a training set and a test set using __train_test_split()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and testing data:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_moon, y_moon, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 135 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 405 out of 405 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'criterion': ['gini'], 'max_leaf_nodes': [2, 5, 10, 50, 100], 'max_depth': [1, 2, 5], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 5, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tune hyper-parameters:\n",
    "\n",
    "#Model\n",
    "desTree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#Parameters\n",
    "tree_param = {\n",
    "    'criterion' : ['gini'],\n",
    "    'max_leaf_nodes': [2, 5, 10, 50, 100],\n",
    "    'max_depth' : [1, 2, 5],\n",
    "    'min_samples_split' : [2, 5, 10],\n",
    "    'min_samples_leaf' : [1, 5, 10],\n",
    "}\n",
    "\n",
    "#Grid-search\n",
    "gs_desTree = GridSearchCV(desTree, tree_param, n_jobs=-1, verbose=1, cv=3)\n",
    "\n",
    "#Fit\n",
    "gs_desTree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Train it on the full training set using these hyperparameters, and measure your model's performance on the test set. You should get roughly 85% to 87% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy Score: 85.45%\n"
     ]
    }
   ],
   "source": [
    "#Predict from test set:\n",
    "y_pred = gs_desTree.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Decision Tree Accuracy Score: {:.2f}%\".format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Grow a forest\n",
    "a. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn's ShuffleSplit class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = 1000\n",
    "instances = 100\n",
    "\n",
    "len_train = len(X_train)\n",
    "len_test = len_train - instances\n",
    "\n",
    "#Generate 100 random index's 1000 times \n",
    "shuffle = ShuffleSplit(n_splits=trees, test_size=len_test, random_state=42)\n",
    "\n",
    "forest_sets = []\n",
    "\n",
    "#Train a tree for each subset:\n",
    "for train_idx, test_idx in shuffle.split(X_train):\n",
    "    X_train_f = X_train[train_idx]\n",
    "    y_train_f = y_train[train_idx]\n",
    "    forest_sets.append((X_train_f, y_train_f))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest Mean Accuracy Score: 79.66%\n"
     ]
    }
   ],
   "source": [
    "#Get best tree model\n",
    "best_tree = gs_desTree.best_estimator_\n",
    "\n",
    "#Clone best tree 1000 times:\n",
    "forest = [clone(best_tree) for _ in range(trees)]\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "#Get score for each tree in forest\n",
    "for tree, (X_train_f, y_train_f) in zip(forest, forest_sets):\n",
    "    tree.fit(X_train_f, y_train_f)\n",
    "    \n",
    "    y_pred = tree.predict(X_test)\n",
    "    acc_score.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "#Get mean accuracy score:\n",
    "f_acc = np.mean(acc_score)\n",
    "print(\"Forest Mean Accuracy Score: {:.2f}%\".format(100*f_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "c. Now comes the magic. For each test set instance, generate the predictions of the 1,000 Decision Trees, and keep only the most frequent prediction (you can use SciPy's mode() function for this). This gives you majority-vote predictions over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Empty array to be filled with each tree's prediction of the test data\n",
    "vote_pred = np.empty([trees, len(X_test)], dtype=np.uint8)\n",
    "\n",
    "#predict test data for each tree\n",
    "for tree_idx, tree in enumerate(forest):\n",
    "    vote_pred[tree_idx] = tree.predict(X_test)\n",
    "\n",
    "#Get number of votes per instance and make prediciton most frequent prediciton\n",
    "y_pred_hard, n_votes = mode(vote_pred, axis=0)\n",
    "y_pred_hard.reshape([-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "d. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a Random Forest classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest (hard) Vote Accuracy Score: 83.30%\n"
     ]
    }
   ],
   "source": [
    "#get accuracy of voting:\n",
    "acc_vote = accuracy_score(y_test, y_pred_hard.reshape([-1]))\n",
    "print(\"Forest (hard) Vote Accuracy Score: {:.2f}%\".format(100*acc_vote))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
