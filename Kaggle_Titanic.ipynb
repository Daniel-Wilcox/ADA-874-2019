{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: Titanic Challenge\n",
    "## Coded by Daniel Wilcox\n",
    "\n",
    "This is a notebook showing the process in predicting the survivors of the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.base import clone\n",
    "\n",
    "#Fills in values to empty data locations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Creating custom Transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Location to save the dataset\n",
    "TITANIC_PATH = \"datasets/titanic\"\n",
    "TITANIC_URL = \"https://github.com/Daniel-Wilcox/ADA-874-2019/blob/master/datasets/titanic/\"\n",
    "train_name = \"train.csv\" \n",
    "test_name = \"test.csv\" \n",
    "\n",
    "\n",
    "#The Location to save the models\n",
    "PICKLE_PATH = \"PickleModels/Titanic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle functions\n",
    "\n",
    "#Saving and storing the model\n",
    "def save_pickle(model_name, model, pic_path=PICKLE_PATH):\n",
    "    print(\"Saving model...\")\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(cwd+\"/\"+pic_path)\n",
    "        \n",
    "    f = open(model_name, \"wb\")\n",
    "    pickle.dump(model, f)\n",
    "    f.close()\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "    print(\"Saved \"+model_name+\" successfully!\\n\")\n",
    "    return None\n",
    "    \n",
    "    \n",
    "#Retrieving and loading the model\n",
    "def load_pickle(model_name, pic_path=PICKLE_PATH):\n",
    "    print(\"Loading \"+model_name+\" from Pickle file...\")\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(cwd+\"/\"+pic_path)\n",
    "    \n",
    "    f = open(model_name, \"rb\")\n",
    "    p = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "    print(model_name+\" successfully loaded!\\n\")\n",
    "    return p\n",
    "\n",
    "#Check whether the pickel exists\n",
    "def pickle_exist(model_name, pic_path=PICKLE_PATH):\n",
    "    #check if pickle file exists\n",
    "    print(\"Checking if pickle directory exists...\")\n",
    "    if not os.path.isdir(pic_path):\n",
    "        os.makedirs(pic_path)\n",
    "        print(\"Directory does NOT exists\")\n",
    "        print(\"Creating directory\")\n",
    "    \n",
    "    else: \n",
    "        print(\"Directory exists\")\n",
    "        \n",
    "    if os.path.isfile(pic_path+\"/\"+model_name):\n",
    "        print(\"Pickle file does exists...\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Pickle file does NOT exists...\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Titanic_data(file_name, titanic_path=TITANIC_PATH):\n",
    "    csv_path = os.path.join(titanic_path, file_name)\n",
    "    return pd.read_csv(csv_path)\n",
    "        \n",
    "    \n",
    "def get_Titanic_data(file_name, titanic_url=TITANIC_URL, titanic_path=TITANIC_PATH):\n",
    "    csv_path = os.path.join(titanic_path, file_name)\n",
    "    \n",
    "    print(\"Checking if directory exists...\")\n",
    "    if not os.path.isdir(titanic_path):\n",
    "        os.makedirs(titanic_path)\n",
    "        print(\"Creating directory\")\n",
    "    \n",
    "    else: \n",
    "        print(\"Directory exists\") \n",
    "            \n",
    "        if os.path.isfile(csv_path):\n",
    "            print(file_name + \" file does exists...\")\n",
    "            print(\"extracting \" + file_name)\n",
    "            \n",
    "            titanic = load_Titanic_data(file_name)\n",
    "            print(\"\\nSuccess!\")\n",
    "            return titanic\n",
    "        \n",
    "        else:\n",
    "            print(file_name + \" file doesn't exists...\")\n",
    "            print(\"Download .csv from Kaggle!\")\n",
    "\n",
    "            return None\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = get_Titanic_data(train_name)\n",
    "Test = get_Titanic_data(test_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Variable - Definition                              - Key\n",
    "1. survival - Survival                                - 0/1 = No/Yes\n",
    "2. pclass   - Ticket class                            - 1,2,3 = 1st, 2nd, 3rd class\n",
    "3. sex      - Sex                                     - male, female\n",
    "4. Age      - Age in years                            - ...\n",
    "5. sibsp    - # of siblings/spouses on the Titanic    - ...\n",
    "6. parch    - # of parents/children on the Titanic    - ...\n",
    "7. ticket   - Ticket number                           - ...\n",
    "8. fare     - Passenger fare                          - ...\n",
    "9. cabin    - Cabin number                            - ...\n",
    "10. embarked - Port of Embarkation                     - C = Cherbourg, Q = Queenstown, S = Southampton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It can be seen that there are some features that have NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sur = Train[\"Survived\"].value_counts() / len(Train)\n",
    "print(\"{:.2f}% Survived\\n{:.2f}% Died\".format(100*sur[1],100*sur[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix (numeric features)\n",
    "corr_matrix = Train.corr()\n",
    "corr_matrix[\"Survived\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pclass\n",
    "\n",
    "fig = sns.barplot(x=\"Pclass\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"Pclass\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability of passanger ticket class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pclass w/ Sex\n",
    "\n",
    "fig = sns.barplot(x=\"Pclass\",y=\"Survived\", hue=\"Sex\", data=Train)\n",
    "fig = fig.set(xlabel=\"Pclass\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability of passanger ticket class (w/ Sex)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex\n",
    "\n",
    "fig = sns.barplot(x=\"Sex\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"Sex\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability of passanger's Sex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age\n",
    "\n",
    "d = {'color': ['r', 'g']}   \n",
    "fig = sns.FacetGrid(Train, col='Survived',  hue_kws=d, hue='Survived')\n",
    "fig = fig.map(sns.distplot, \"Age\")   \n",
    "\n",
    "fig = fig.set(xlabel=\"Age\", ylabel=\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SibSp\n",
    "\n",
    "fig = sns.barplot(x=\"SibSp\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"SibSp\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for number of Siblings/Spouses of passenger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parch\n",
    "\n",
    "fig = sns.barplot(x=\"Parch\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"Parch\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for number of Parents/Children of passenger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fare\n",
    "\n",
    "d = {'color': ['r', 'g']}   \n",
    "fig = sns.FacetGrid(Train, col='Survived',  hue_kws=d, hue='Survived')\n",
    "fig = fig.map(sns.distplot, \"Fare\")   \n",
    "\n",
    "fig = fig.set(xlabel=\"Fare\", ylabel=\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embarked\n",
    "\n",
    "fig = sns.barplot(x=\"Embarked\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"Embarked\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability of passanger port of embarkation \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature eng:\n",
    "\n",
    "#tr = Train.drop(\"Survived\", axis=1)\n",
    "#Full_set = pd.concat(objs=[tr, Test], axis=0).reset_index(drop=True)\n",
    "Full_set = pd.concat(objs=[Train, Test], axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#Name: f_name, honorifics. sur_name\n",
    "honorifics = [i.split(\",\")[1].split(\".\")[0].strip() for i in Full_set[\"Name\"]]\n",
    "Full_set[\"Title\"] = pd.Series(honorifics)\n",
    "Full_set.Title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.countplot(x=\"Title\",data=Full_set)\n",
    "#fig = fig.set(xlabel=\"Title\", ylabel=\"Survival Probability\")\n",
    "fig = plt.setp(fig.get_xticklabels(), rotation=80) \n",
    "fig = plt.title(\"Count of honorifics (Trainin + Testing)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_set[\"Title\"] = Full_set[\"Title\"].replace(['Don', \n",
    "        'Rev', 'Dr', 'Mme', 'Major', 'Lady', \n",
    "        'Sir', 'Mlle', 'Col', 'Capt', 'the Countess',\n",
    "        'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "Full_set[\"Title\"] = Full_set[\"Title\"].replace(['Mrs',\n",
    "        'Miss', 'Ms'], 'Girls/Women')\n",
    "\n",
    "Full_set[\"Title\"] = Full_set[\"Title\"].replace(['Master'],\n",
    "        'Boys')\n",
    "\n",
    "Full_set[\"Title\"] = Full_set[\"Title\"].replace(['Mr'],\n",
    "        'Men')\n",
    "fig = sns.barplot(x=\"Title\",y=\"Survived\",data=Full_set)\n",
    "fig = fig.set(xlabel=\"Honorific Title\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for honorific titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Family size\n",
    "\n",
    "Full_set[\"Fam_size\"] = Full_set[\"SibSp\"] + Full_set[\"Parch\"] + 1\n",
    "\n",
    "fig = sns.barplot(x=\"Fam_size\",y=\"Survived\", data=Full_set)\n",
    "fig = fig.set(xlabel=\"Fam_size\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for family size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alone\n",
    "Full_set[\"Alone\"] = 1\n",
    "Full_set[\"Alone\"].loc[Full_set['Fam_size'] > 1] = 0\n",
    "\n",
    "fig = sns.barplot(x=\"Alone\",y=\"Survived\", data=Full_set)\n",
    "fig = fig.set(xlabel=\"Alone\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for alone passangers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separtate labels from features\n",
    "y_train = Train[\"Survived\"].copy()\n",
    "X_tr = Train.drop(\"Survived\", axis=1)\n",
    "X_tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Features (survived is already removed)\n",
    "list_drop = ['Name', 'Ticket', 'Cabin']\n",
    "list_excl = ['Ticket', 'Cabin'] \n",
    "\n",
    "add_fam_feat = True\n",
    "add_alone = True \n",
    "add_title = True\n",
    "    \n",
    "Dropped = X_tr[list(set(X_tr.columns) - set(list_drop))]\n",
    "\n",
    "#Numeric Features\n",
    "list_num = Dropped.select_dtypes(include = [\"number\"]).columns\n",
    "\n",
    "#Catagorical Features (to be transformed into OHE)\n",
    "list_cat = Dropped.select_dtypes(include = [\"object\"]).columns\n",
    "\n",
    "list_add = X_tr[list(set(X_tr.columns) - set(list_excl))].columns\n",
    "\n",
    "\n",
    "if add_fam_feat:\n",
    "    list_num = list_num.insert(len(list_num)+ 1,'Fam_size')\n",
    "    \n",
    "if add_alone:\n",
    "    list_num = list_num.insert(len(list_num)+1,'Alone')    \n",
    "    \n",
    "if add_title:\n",
    "    list_cat = list_cat.insert(len(list_num)+1,'Title')\n",
    "    \n",
    "\n",
    "print('list_num: {}'.format(list(list_num)))\n",
    "print('list_cat: {}'.format(list(list_cat)))\n",
    "print('list_add: {}'.format(list(list_add)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.feature_names].values\n",
    "    \n",
    "    \n",
    "    \n",
    "class add_features(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, added_feat, add_fam=True, \n",
    "                 add_alone=True, add_title=True):\n",
    "        \n",
    "        self.added_feat = added_feat\n",
    "        self.add_fam = add_fam\n",
    "        self.add_alone = add_alone\n",
    "        self.add_title = add_title\n",
    "   \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return(self)\n",
    "\n",
    "    \n",
    "    def transform(self, X):  \n",
    "        df = X[self.added_feat]\n",
    "        \n",
    "        if self.add_fam:\n",
    "            df[\"Fam_size\"] = df.loc[:,\"SibSp\"] + df.loc[:,\"Parch\"] + 1\n",
    "            \n",
    "        if self.add_alone:\n",
    "            df[\"Alone\"] = 0\n",
    "            df[\"Alone\"].loc[(df[\"SibSp\"]==0) & \n",
    "                           (df[\"Parch\"]==0)] = 1\n",
    "            \n",
    "        if self.add_title:\n",
    "            honorifics = [i.split(\",\")[1].split(\".\")[0].\n",
    "                          strip() for i in df[\"Name\"]]\n",
    "            \n",
    "            df[\"Title\"] = pd.Series(honorifics)\n",
    "            \n",
    "            df[\"Title\"] = df.loc[:,\"Title\"].replace(\n",
    "                ['Don', 'Rev', 'Dr', 'Mme', 'Major', \n",
    "                 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', \n",
    "                 'the Countess', 'Jonkheer', 'Dona'],\n",
    "                 'Rare')\n",
    "\n",
    "            df[\"Title\"] = df.loc[:,\"Title\"].replace(\n",
    "                ['Mrs','Miss','Ms'], 'Girls/Women')\n",
    "\n",
    "            df[\"Title\"] = df.loc[:,\"Title\"].replace(\n",
    "                ['Master'], 'Boys')\n",
    "\n",
    "            df[\"Title\"] = df.loc[:,\"Title\"].replace(\n",
    "                ['Mr'], 'Men') \n",
    "            \n",
    "            df.drop(labels=[\"Name\"], axis = 1, \n",
    "                   inplace = True)\n",
    "\n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Features\n",
    "add_pipeline = Pipeline([\n",
    "    ('add_feat', add_features(list_add))\n",
    "])\n",
    "\n",
    "#Numeric Transformations\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', Selector(list_num)),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "#Catagorical Transformations\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', Selector(list_cat)),\n",
    "    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ('cat_encoder', OneHotEncoder(sparse=False)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = add_pipeline.fit_transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Transformed Dataframe: {}'.format(list(a.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perpare_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline)\n",
    "])\n",
    "\n",
    "full_pipe = Pipeline([\n",
    "    (\"add_pipeline\", add_pipeline),\n",
    "    (\"prep_pipeline\", perpare_pipeline)\n",
    "])\n",
    "\n",
    "X_train = full_pipe.fit_transform(X_tr)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled Test Set for predictions:\n",
    "X_test = full_pipe.fit_transform(Test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create new model if pickle already even if pickle exists; load_pkl = False\n",
    "load_pkl = True\n",
    "\n",
    "cv_split=StratifiedKFold(n_splits=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cross_val(clf, name_clf, lp=load_pickle, cv=cv_split):\n",
    "    if pickle_exist(str(name_clf)) and lp:\n",
    "        ml_clf = load_pickle(str(name_clf))\n",
    "    else:\n",
    "        ml_clf = clf\n",
    "        ml_clf.fit(X_train, y_train)\n",
    "        save_pickle(str(name_clf), ml_clf)\n",
    "    cvs =  cross_val_score(ml_clf, X_train, y_train, cv=3, scoring=\"accuracy\") \n",
    "    return  np.mean(cvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_options = [          \n",
    "    LinearSVC(),\n",
    "    SVC(),\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    SGDClassifier(),\n",
    "    KNeighborsClassifier()\n",
    "    ]          \n",
    " \n",
    "compare_col = ['Clf Name', 'Clf Parameters', 'Clf Mean Accuracy'] \n",
    "clf_compare = pd.DataFrame(columns = compare_col)\n",
    "          \n",
    "          \n",
    "row=0\n",
    "          \n",
    "for clf in clf_options: \n",
    "    clf_name = clf.__class__.__name__\n",
    "                    \n",
    "    clf_compare.loc[row, 'Clf Name'] = clf_name\n",
    "    clf_compare.loc[row, 'Clf Parameters'] = str(clf.get_params()) \n",
    "    cvs = model_cross_val(clf, clf_name)\n",
    "    \n",
    "    clf_compare.loc[row, 'Clf Mean Accuracy'] = cvs\n",
    "    \n",
    "    row +=1\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_compare.sort_values(by = 'Clf Mean Accuracy', ascending = False, inplace = True)\n",
    "clf_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters of classifiers (Grid Search):\n",
    "\n",
    "#Linear SVC\n",
    "lin_svc_param = {\n",
    "    'loss':['hinge','squared_hinge'],\n",
    "    'C':[1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "#SVC\n",
    "svc_param = {\n",
    "    'C' : [1, 2, 5, 10],\n",
    "    'kernel' : ['linear', 'rbf'],\n",
    "    'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "#Logistic Regression\n",
    "log_reg_param = {\n",
    "    'C': [1, 2, 5, 10],\n",
    "    'penalty' : ['l1', 'l2']\n",
    "}\n",
    "\n",
    "#Decision Tree Classifier\n",
    "tree_param = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'splitter' : ['best', 'random'],\n",
    "    'max_depth' : [1, 2, 3, 5]\n",
    "}\n",
    "\n",
    "#Random Forest Classifier\n",
    "forest_param = {\n",
    "    'n_estimators' : [100, 200, 500],\n",
    "    'criterion' : ['gini'],\n",
    "    'max_features' : [1, 3, 5, 10],\n",
    "    'min_samples_split' : [2, 3, 5, 10],\n",
    "    'min_samples_leaf' : [1, 2, 3, 10]\n",
    "}\n",
    "\n",
    "#SGDClassifier\n",
    "SGDC_param = {\n",
    "    'alpha':[0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "#K-Neighbors Classifier\n",
    "k_neigh_param = {\n",
    "    'n_neighbors': [3, 4, 5],\n",
    "    'weights': ['uniform','distance']\n",
    "}\n",
    "\n",
    "param_option_gs = [\n",
    "    lin_svc_param,\n",
    "    svc_param,\n",
    "    log_reg_param,\n",
    "    tree_param,\n",
    "    forest_param,\n",
    "    SGDC_param,\n",
    "    k_neigh_param \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters of classifiers (Random Search):\n",
    "\n",
    "#Linear SVC\n",
    "lin_svc_rs = {\n",
    "    'loss':('hinge','squared_hinge'),\n",
    "    'C':[1, 100]\n",
    "}\n",
    "\n",
    "#SVC\n",
    "svc_rs = {\n",
    "    'C' : [1, 100],\n",
    "    'kernel' : ['linear', 'rbf'],\n",
    "    'gamma': [ 0.001, 10],\n",
    "}\n",
    "\n",
    "#Logistic Regression\n",
    "log_reg_rs = {\n",
    "    'C': [1, 100],\n",
    "    'penalty' : ['l1', 'l2']\n",
    "}\n",
    "\n",
    "#Decision Tree Classifier\n",
    "tree_rs = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'splitter' : ['best', 'random'],\n",
    "    'max_depth' : [1, 15]\n",
    "}\n",
    "\n",
    "#Random Forest Classifier\n",
    "forest_rs = {\n",
    "    'n_estimators' : [100, 1000],\n",
    "    'criterion' : ['gini'],\n",
    "    'min_samples_split' : [2, 15],\n",
    "    'min_samples_leaf' : [1, 15]\n",
    "}\n",
    "\n",
    "#SGDClassifier\n",
    "SGDC_rs = {\n",
    "    'alpha':[0.0001, 10]\n",
    "}\n",
    "\n",
    "#K-Neighbors Classifier\n",
    "k_neigh_rs = {\n",
    "    'n_neighbors': [2, 10],\n",
    "    'weights': ['uniform','distance']\n",
    "}\n",
    "\n",
    "param_option_rs = [\n",
    "    lin_svc_rs,\n",
    "    svc_rs,\n",
    "    log_reg_rs,\n",
    "    tree_rs,\n",
    "    forest_rs,\n",
    "    SGDC_rs,\n",
    "    k_neigh_rs \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "def model_rand_gs(clf, name_clf, clf_param, lp=load_pickle, cv=cv_split): \n",
    "\n",
    "    rand_clf = RandomizedSearchCV(clf, param_distributions=clf_param, cv=cv_split, \n",
    "                          verbose=2, n_jobs=-1, n_iter=15000, scoring='accuracy')\n",
    "    \n",
    "    rand_clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_est = rand_clf.best_estimator_\n",
    "    best_sco = rand_clf.best_score_ \n",
    "    \n",
    "    text_check = os.path.isfile(PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\")\n",
    "    \n",
    "    if pickle_exist(\"best_\"+str(name_clf)) and text_check and lp:\n",
    "        #load current best score\n",
    "        prev_best_score = max(np.loadtxt(\n",
    "            (PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\"), dtype=float))\n",
    "\n",
    "        if best_sco > prev_best_score:\n",
    "            temp = [best_sco, best_sco]\n",
    "            np.savetxt(\n",
    "                PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\", temp, fmt='%f')\n",
    "            save_pickle(\"best_\"+str(name_clf), best_est)\n",
    "        else:\n",
    "            #load in better parameters\n",
    "            print(\"****loaded****\")\n",
    "            best_sco = max(np.loadtxt(\n",
    "                PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\", dtype=float))\n",
    "            best_est = load_pickle(\"best_\"+str(name_clf))\n",
    "    else:\n",
    "        #make pickles if dont exist\n",
    "        temp = [best_sco, best_sco]\n",
    "        np.savetxt(\n",
    "            PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\", temp, fmt='%f')\n",
    "        save_pickle(\"best_\"+str(name_clf), best_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_grid(clf, name_clf, clf_param, lp=load_pickle, cv=cv_split): \n",
    "\n",
    "    gs_clf = GridSearchCV(clf, param_grid=clf_param, cv=cv_split, \n",
    "                          verbose=2, n_jobs=-1, scoring='accuracy')\n",
    "    \n",
    "    gs_clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_est = gs_clf.best_estimator_\n",
    "    best_sco = gs_clf.best_score_ \n",
    "    \n",
    "    text_check = os.path.isfile(PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\")\n",
    "    \n",
    "    if pickle_exist(\"best_\"+str(name_clf)) and text_check and lp:\n",
    "        #load current best score\n",
    "        prev_best_score = max(np.loadtxt((PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\"), dtype=float))\n",
    "\n",
    "        if best_sco > prev_best_score:\n",
    "            temp = [best_sco, best_sco]\n",
    "            np.savetxt(PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\", temp, fmt='%f')\n",
    "            save_pickle(\"best_\"+str(name_clf), best_est)\n",
    "        else:\n",
    "            #load in better parameters\n",
    "            best_sco = max(np.loadtxt(PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\", dtype=float))\n",
    "            best_est = load_pickle(\"best_\"+str(name_clf))\n",
    "    else:\n",
    "        #make pickles if dont exist\n",
    "        temp = [best_sco, best_sco]\n",
    "        np.savetxt(PICKLE_PATH+\"/best_score_\"+str(name_clf)+\".txt\", temp, fmt='%f')\n",
    "        save_pickle(\"best_\"+str(name_clf), best_est)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_col = ['Clf Name', 'Best Clf Parameters', 'Best Clf Accuracy Score'] \n",
    "best_compare = pd.DataFrame(columns = best_col)\n",
    "          \n",
    "          \n",
    "row=0\n",
    "\n",
    "#for clf, param in zip(clf_options, param_option_gs): \n",
    "for clf, param in zip(clf_options, param_option_rs): \n",
    "    \n",
    "    clf_name = clf.__class__.__name__\n",
    "    best_compare.loc[row, 'Clf Name'] = clf_name\n",
    "    \n",
    "    print(\"{}: {}\".format(row, clf_name))\n",
    "    \n",
    "    #model_grid(clf, clf_name, param)\n",
    "    model_rand_gs(clf, clf_name, param)\n",
    "    \n",
    "    best_score = max(np.loadtxt((PICKLE_PATH+\"/best_score_\"+str(clf_name)+\".txt\"), dtype=float))\n",
    "    best_clf = load_pickle(\"best_\"+str(clf_name))\n",
    "    \n",
    "    best_compare.loc[row, 'Best Clf Parameters'] = str(best_clf.get_params())\n",
    "    best_compare.loc[row, 'Best Clf Accuracy Score'] = str(best_score)\n",
    "    \n",
    "    row +=1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before Grid-search\n",
    "clf_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Gridseach\n",
    "best_compare.sort_values(by = 'Best Clf Accuracy Score', ascending = False, inplace = True)\n",
    "best_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best(clf):\n",
    "    clf_name = clf.__class__.__name__\n",
    "    return load_pickle(\"best_\"+str(clf_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, ver_index in split.split(Train, Train[\"Survived\"]):\n",
    "    Train_strat = Train.loc[train_index]\n",
    "    Ver_strat = Train.loc[ver_index]\n",
    "\n",
    "print(\"Training set: {} entries, Verificaiton set: {} entries\".format(len(Train_strat),len(Ver_strat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training = Train_strat[\"Survived\"].copy()\n",
    "X_tr = Train_strat.drop(\"Survived\", axis=1)\n",
    "X_training = perpare_pipeline.fit_transform(X_tr)\n",
    "\n",
    "y_verification = Ver_strat[\"Survived\"].copy()\n",
    "X_ver = Ver_strat.drop(\"Survived\", axis=1)\n",
    "X_verification = perpare_pipeline.fit_transform(X_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_svc = load_best(clf_options[0])\n",
    "svc = load_best(clf_options[1])\n",
    "log_reg = load_best(clf_options[2])\n",
    "tree = load_best(clf_options[3])\n",
    "forest = load_best(clf_options[4])\n",
    "SGDC = load_best(clf_options[5])\n",
    "knn = load_best(clf_options[6])\n",
    "\n",
    "\n",
    "est = [('lin_svc', lin_svc), ('svc', svc), ('log_reg', log_reg),\n",
    "       ('tree', tree), ('forest', forest), ('SGDC', SGDC),\n",
    "       ('knn', knn)]\n",
    "\n",
    "vote_clf = VotingClassifier(estimators=est, voting='hard')\n",
    "\n",
    "vote_clf.fit(X_training, y_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in (lin_svc, svc, log_reg, tree, \n",
    "            forest, SGDC, knn, vote_clf):\n",
    "    \n",
    "    clf.fit(X_training, y_training)\n",
    "    y_pred = clf.predict(X_verification)\n",
    "    \n",
    "    name = clf.__class__.__name__\n",
    "    score = accuracy_score(y_verification, y_pred)\n",
    "    \n",
    "    print(\"{}: {:.2f}%\".format(name, 100*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit to whole dataset\n",
    "vote_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(csv_name, save_loc=TITANIC_PATH):\n",
    "    curr_path = os.getcwd()\n",
    "    save_path = os.path.join(curr_path, save_loc)\n",
    "    os.chdir(save_path)\n",
    "    \n",
    "    max_i = 0\n",
    "    \n",
    "    len_name = len(csv_name)\n",
    "           \n",
    "    for file in glob.glob(csv_name+'*.csv'):\n",
    "        \n",
    "        file_name = file[:len(file)-4]\n",
    "        file_ver = file_name[len_name:]\n",
    "        \n",
    "        if int(file_ver) > max_i:\n",
    "            max_i = int(file_ver)\n",
    "        \n",
    "    new_ver = csv_name+str(max_i+1)+'.csv'\n",
    "        \n",
    "        \n",
    "        \n",
    "    os.chdir(curr_path)\n",
    "    \n",
    "    return os.path.join(save_path, new_ver)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PassengerId = Test['PassengerId']\n",
    "\n",
    "Survived_pred = vote_clf.predict(X_test) \n",
    "\n",
    "\n",
    "Submission = pd.DataFrame({ 'PassengerId': PassengerId,\n",
    "                            'Survived': Survived_pred })\n",
    "\n",
    "name = \"Submission\"\n",
    "file_name = make_csv(name)\n",
    "\n",
    "Submission.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
