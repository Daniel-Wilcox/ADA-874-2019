{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle: Titanic Challenge\n",
    "## Coded by Daniel Wilcox\n",
    "\n",
    "This is a notebook showing the process in predicting the survivors of the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import randint, reciprocal, expon, uniform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.base import clone\n",
    "\n",
    "#Fills in values to empty data locations\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Creating custom Transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "#Classifier Models:\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import NuSVC #-----\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier #-----\n",
    "from sklearn.linear_model import Perceptron #-----\n",
    "from sklearn.linear_model import RidgeClassifier #-----\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier #-----\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB #-----\n",
    "from sklearn.naive_bayes import GaussianNB #-----\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier #-----\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis #-----\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis #-----\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier #-----\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Location to save the dataset\n",
    "TITANIC_PATH = \"datasets/titanic\"\n",
    "TITANIC_URL = \"https://github.com/Daniel-Wilcox/ADA-874-2019/blob/master/datasets/titanic/\"\n",
    "train_name = \"train.csv\" \n",
    "test_name = \"test.csv\" \n",
    "\n",
    "\n",
    "#The Location to save the models\n",
    "PICKLE_PATH = \"PickleModels/Titanic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickle functions\n",
    "\n",
    "#Saving and storing the model\n",
    "def save_pickle(model_name, model, pic_path=PICKLE_PATH):\n",
    "    print(\"Saving model...\")\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(cwd+\"/\"+pic_path)\n",
    "        \n",
    "    f = open(model_name, \"wb\")\n",
    "    pickle.dump(model, f)\n",
    "    f.close()\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "    print(\"Saved \"+model_name+\" successfully!\\n\")\n",
    "    return None\n",
    "    \n",
    "    \n",
    "#Retrieving and loading the model\n",
    "def load_pickle(model_name, pic_path=PICKLE_PATH):\n",
    "    print(\"Loading \"+model_name+\" from Pickle file...\")\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(cwd+\"/\"+pic_path)\n",
    "    \n",
    "    f = open(model_name, \"rb\")\n",
    "    p = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    os.chdir(cwd)\n",
    "    print(model_name+\" successfully loaded!\\n\")\n",
    "    return p\n",
    "\n",
    "#Check whether the pickel exists\n",
    "def pickle_exist(model_name, pic_path=PICKLE_PATH):\n",
    "    #check if pickle file exists\n",
    "    print(\"Checking if pickle directory exists...\")\n",
    "    if not os.path.isdir(pic_path):\n",
    "        os.makedirs(pic_path)\n",
    "        print(\"Directory does NOT exists\")\n",
    "        print(\"Creating directory\")\n",
    "    \n",
    "    else: \n",
    "        print(\"Directory exists\")\n",
    "        \n",
    "    if os.path.isfile(pic_path+\"/\"+model_name):\n",
    "        print(\"Pickle file does exists...\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Pickle file does NOT exists...\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Titanic_data(file_name, titanic_path=TITANIC_PATH):\n",
    "    csv_path = os.path.join(titanic_path, file_name)\n",
    "    return pd.read_csv(csv_path)\n",
    "        \n",
    "    \n",
    "def get_Titanic_data(file_name, titanic_url=TITANIC_URL, titanic_path=TITANIC_PATH):\n",
    "    csv_path = os.path.join(titanic_path, file_name)\n",
    "    \n",
    "    print(\"Checking if directory exists...\")\n",
    "    if not os.path.isdir(titanic_path):\n",
    "        os.makedirs(titanic_path)\n",
    "        print(\"Creating directory\")\n",
    "    \n",
    "    else: \n",
    "        print(\"Directory exists\") \n",
    "            \n",
    "        if os.path.isfile(csv_path):\n",
    "            print(file_name + \" file does exists...\")\n",
    "            print(\"extracting \" + file_name)\n",
    "            \n",
    "            titanic = load_Titanic_data(file_name)\n",
    "            print(\"\\nSuccess!\")\n",
    "            return titanic\n",
    "        \n",
    "        else:\n",
    "            print(file_name + \" file doesn't exists...\")\n",
    "            print(\"Download .csv from Kaggle!\")\n",
    "\n",
    "            return None\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notify(title, text):\n",
    "    os.system(\"\"\"\n",
    "              osascript -e 'display notification \"{}\" with title \"{}\"'\n",
    "              \"\"\".format(text, title))\n",
    "    os.system('osascript -e \"beep 1\"')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = get_Titanic_data(train_name)\n",
    "Test = get_Titanic_data(test_name)\n",
    "\n",
    "titanic = pd.concat(objs=[Train, Test], axis=0, sort=False).reset_index(drop=True)\n",
    "\n",
    "#Fill \"survived\" of test data in titanic as 0:\n",
    "titanic[\"Survived\"] = titanic[\"Survived\"].fillna(0)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Variable - Definition                              - Key\n",
    "1. survival - Survival                                - 0/1 = No/Yes\n",
    "2. pclass   - Ticket class                            - 1,2,3 = 1st, 2nd, 3rd class\n",
    "3. sex      - Sex                                     - male, female\n",
    "4. Age      - Age in years                            - ...\n",
    "5. sibsp    - # of siblings/spouses on the Titanic    - ...\n",
    "6. parch    - # of parents/children on the Titanic    - ...\n",
    "7. ticket   - Ticket number                           - ...\n",
    "8. fare     - Passenger fare                          - ...\n",
    "9. cabin    - Cabin number                            - ...\n",
    "10. embarked - Port of Embarkation                     - C = Cherbourg, Q = Queenstown, S = Southampton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sur = Train[\"Survived\"].value_counts() / len(Train)\n",
    "print(\"From the Train.csv dataset provided:\")\n",
    "print(\"{:.2f}% Survived\\n{:.2f}% Died\".format(100*sur[1],100*sur[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation matrix (numeric features)\n",
    "corr_matrix = Train.corr()\n",
    "corr_matrix[\"Survived\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age: fill NaN's\n",
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "\n",
    "#Cabin:\n",
    "titanic[\"Cabin\"] = titanic[\"Cabin\"].fillna('U')\n",
    "titanic[\"Cabin\"] = titanic[\"Cabin\"].map(lambda x: x[0])\n",
    "\n",
    "#Embarked: fill\n",
    "most_embarked = titanic[\"Embarked\"].value_counts().index[0]\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(most_embarked)\n",
    "\n",
    "#Fare: fill \n",
    "titanic[\"Fare\"] = titanic[\"Fare\"].fillna(titanic[\"Fare\"].median())\n",
    "\n",
    "#Name:\n",
    "form_name = lambda x: x.split(',')[1].split('.')[0].strip()\n",
    "titanic[\"Title\"] = titanic[\"Name\"].map(form_name)\n",
    "\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace(['Don', \n",
    "        'Rev', 'Dr', 'Mme', 'Major', 'Lady', \n",
    "        'Sir', 'Mlle', 'Col', 'Capt', 'the Countess',\n",
    "        'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace(['Mrs',\n",
    "        'Miss', 'Ms'], 'Girls/Women')\n",
    "\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace(['Master'],\n",
    "        'Boys')\n",
    "\n",
    "titanic[\"Title\"] = titanic[\"Title\"].replace(['Mr'],\n",
    "        'Men')\n",
    "\n",
    "titanic[\"Alone\"] = 0\n",
    "titanic[\"Alone\"].loc[(titanic[\"SibSp\"]==0) & (titanic[\"Parch\"]==0)] = 1\n",
    "\n",
    "titanic[\"Fam_size\"] = titanic.loc[:,\"SibSp\"] + titanic.loc[:,\"Parch\"] + 1\n",
    "titanic[\"fare_per_fam\"] = titanic[\"Fare\"]/titanic[\"Fam_size\"]\n",
    "            \n",
    "        \n",
    "form_sname = lambda x: x.split(',')[0].strip()\n",
    "titanic[\"surname\"] = titanic[\"Name\"].map(form_sname)\n",
    "\n",
    "titanic[\"short_Ticket\"] = titanic[\"Ticket\"].str[:-2]\n",
    "\n",
    "f_Id = lambda x: '-'.join(x.map(str))\n",
    "titanic[\"Fam_ID\"] = titanic[[\"surname\", \"Pclass\", \"Embarked\", \"Cabin\", \"short_Ticket\"]].apply(f_Id, axis=1)\n",
    "titanic[\"Fam_ID\"].loc[(titanic[\"Alone\"] == 1)] = 'Alone'\n",
    "\n",
    "#age_group\n",
    "titanic[\"age_group\"] = pd.qcut(titanic.Age, q=4, labels=False)        \n",
    "    \n",
    "#fare_group\n",
    "titanic[\"Fare_group\"] = pd.qcut(titanic.Fare, q=6, labels=False) \n",
    "\n",
    "\n",
    "titanic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pclass\n",
    "\n",
    "fig = sns.barplot(x=\"Pclass\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"Pclass\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability of passanger ticket class\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pclass w/ Sex\n",
    "\n",
    "fig = sns.barplot(x=\"Pclass\",y=\"Survived\", hue=\"Sex\", data=Train)\n",
    "fig = fig.set(xlabel=\"Pclass\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability of passanger ticket class (w/ Sex)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sex\n",
    "fig = sns.barplot(x=\"Sex\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"Sex\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability of passanger's Sex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age\n",
    "\n",
    "d = {'color': ['r', 'g']}   \n",
    "fig = sns.FacetGrid(Train, col='Survived',  hue_kws=d, hue='Survived')\n",
    "fig = fig.map(sns.distplot, \"Age\")   \n",
    "\n",
    "fig = fig.set(xlabel=\"Age\", ylabel=\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SibSp\n",
    "\n",
    "fig = sns.barplot(x=\"SibSp\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"SibSp\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for number of Siblings/Spouses of passenger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parch\n",
    "\n",
    "fig = sns.barplot(x=\"Parch\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"Parch\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for number of Parents/Children of passenger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fare\n",
    "\n",
    "d = {'color': ['r', 'g']}   \n",
    "fig = sns.FacetGrid(Train, col='Survived',  hue_kws=d, hue='Survived')\n",
    "fig = fig.map(sns.distplot, \"Fare\")   \n",
    "\n",
    "fig = fig.set(xlabel=\"Fare\", ylabel=\"Survival Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cabin\n",
    "fig = sns.countplot(x=\"Cabin\",data=titanic)\n",
    "fig = plt.setp(fig.get_xticklabels(), rotation=80) \n",
    "fig = plt.title(\"Count of cabin prefix (whole titanic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_cabin = ['A','B','C','D','E','F','G','T','U']\n",
    "fig = sns.factorplot(x=\"Cabin\", y=\"Survived\", data=titanic,\n",
    "                    kind=\"bar\", order=order_cabin)\n",
    "fig = plt.title(\"Survival Probability of cabin prefix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embarked\n",
    "\n",
    "fig = sns.barplot(x=\"Embarked\",y=\"Survived\",data=Train)\n",
    "fig = fig.set(xlabel=\"Embarked\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability of passanger port of embarkation \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature eng:\n",
    "Full_set = pd.concat(objs=[Train, Test], axis=0).reset_index(drop=True)\n",
    "\n",
    "#Name: f_name, honorifics. sur_name\n",
    "honorifics = [i.split(\",\")[1].split(\".\")[0].strip() for i in Full_set[\"Name\"]]\n",
    "Full_set[\"Title\"] = pd.Series(honorifics)\n",
    "Full_set.Title.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.countplot(x=\"Title\",data=Full_set)\n",
    "fig = plt.setp(fig.get_xticklabels(), rotation=80) \n",
    "fig = plt.title(\"Count of honorifics (whole titanic)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.barplot(x=\"Title\",y=\"Survived\",data=titanic)\n",
    "fig = fig.set(xlabel=\"Honorific Title\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for honorific titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Family size\n",
    "\n",
    "Full_set[\"Fam_size\"] = Full_set[\"SibSp\"] + Full_set[\"Parch\"] + 1\n",
    "\n",
    "fig = sns.barplot(x=\"Fam_size\",y=\"Survived\", data=Full_set)\n",
    "fig = fig.set(xlabel=\"Fam_size\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for family size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alone\n",
    "Full_set[\"Alone\"] = 1\n",
    "Full_set[\"Alone\"].loc[Full_set['Fam_size'] > 1] = 0\n",
    "\n",
    "fig = sns.barplot(x=\"Alone\",y=\"Survived\", data=Full_set)\n",
    "fig = fig.set(xlabel=\"Alone\", ylabel=\"Survival Probability\")\n",
    "fig = plt.title(\"Survival probability for alone passangers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separtate labels from features\n",
    "y_train = Train[\"Survived\"].copy()\n",
    "X_tr = Train.drop(\"Survived\", axis=1)\n",
    "X_tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features to add: \n",
    "fix_cabin=True #'A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'U' -num\n",
    "has_cabin=True #has_cabin -num\n",
    "add_fam=True #Fam_size -num\n",
    "add_alone=True #Alone -num\n",
    "fare_per_fam=True #fare_per_fam -num\n",
    "add_title=True #Title -cat\n",
    "add_famid=True #Fam_ID -num\n",
    "add_ageG=True #age_group -num      \n",
    "add_fareG=True #Fare_group -num\n",
    "\n",
    "  \n",
    "#Remove Features (survived is already removed)\n",
    "exclude_col = ['Name', 'Ticket', 'Cabin']\n",
    "  \n",
    "Dropped = X_tr[list(set(X_tr.columns) - set(exclude_col))]\n",
    "\n",
    "#Numeric Features\n",
    "list_num = Dropped.select_dtypes(include = [\"number\"]).columns\n",
    "\n",
    "#Catagorical Features (to be transformed into OHE)\n",
    "list_cat = Dropped.select_dtypes(include = [\"object\"]).columns\n",
    "\n",
    "list_add = X_tr[list(set(X_tr.columns))].columns\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "if fix_cabin:\n",
    "    new_cabin = 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'U'\n",
    "    for i in new_cabin:\n",
    "        list_num = list_num.insert(len(list_num)+1, i)\n",
    "    \n",
    "if has_cabin:\n",
    "    list_num = list_num.insert(len(list_num)+1, 'has_cabin')\n",
    "    \n",
    "if add_fam:\n",
    "    list_num = list_num.insert(len(list_num)+1, 'Fam_size')\n",
    "    \n",
    "if add_alone:\n",
    "    list_num = list_num.insert(len(list_num)+1, 'Alone')\n",
    "\n",
    "if fare_per_fam:\n",
    "    list_num = list_num.insert(len(list_num)+1, 'fare_per_fam')\n",
    "\n",
    "if add_title:\n",
    "    list_cat = list_cat.insert(len(list_cat)+1, 'Title')\n",
    "\n",
    "#if add_famid:\n",
    "    #list_num = list_num.insert(len(list_num)+1, 'Fam_ID')\n",
    "    \n",
    "if add_ageG:\n",
    "    list_num = list_num.insert(len(list_num)+1, 'age_group')\n",
    "    \n",
    "if add_fareG:\n",
    "    list_num = list_num.insert(len(list_num)+1, 'Fare_group')\n",
    "\n",
    "\n",
    " \n",
    "print('list_num: {}\\n'.format(list(list_num)))\n",
    "print('list_cat: {}\\n'.format(list(list_cat)))\n",
    "print('list_add: {}\\n'.format(list(list_add)))\n",
    "list_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr[\"Name\"].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return(self)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.feature_names].values\n",
    "    \n",
    "    \n",
    "#-------------------------------------------------------------------------------    \n",
    "class add_features(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, added_feat, fix_cabin=True, has_cabin=True,\n",
    "                 add_fam=True, add_alone=True, fare_per_fam=True,\n",
    "                 add_title=True, add_famid=True, add_ageG=True,\n",
    "                 add_fareG=True):\n",
    "        \n",
    "        self.added_feat = added_feat\n",
    "        \n",
    "        self.fix_cabin = fix_cabin\n",
    "        self.has_cabin = has_cabin\n",
    "        self.add_fam = add_fam\n",
    "        self.add_alone = add_alone\n",
    "        self.fare_per_fam = fare_per_fam\n",
    "        self.add_title = add_title    \n",
    "        self.add_famid = add_famid\n",
    "        self.add_ageG = add_ageG       \n",
    "        self.add_fareG = add_fareG\n",
    "                \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return(self)\n",
    "\n",
    "    \n",
    "    def transform(self, X):  \n",
    "        df = X[self.added_feat]\n",
    "        \n",
    "        if self.fix_cabin:\n",
    "            df[\"Cabin\"] = df[\"Cabin\"].fillna('U')\n",
    "            df[\"Cabin\"] = df[\"Cabin\"].map(lambda x: x[0])\n",
    "            \n",
    "            for i in ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'U'):\n",
    "                df[i] = 0\n",
    "                df[i].loc[df[\"Cabin\"] == i] = 1\n",
    "            \n",
    "            \n",
    "        if self.has_cabin:\n",
    "            df[\"has_cabin\"] = 1\n",
    "            df[\"has_cabin\"].loc[df[\"Cabin\"] == 'U'] = 0\n",
    "            \n",
    "            \n",
    "        if self.add_fam:\n",
    "            df[\"Fam_size\"] = df.loc[:,\"SibSp\"] + df.loc[:,\"Parch\"] + 1\n",
    "            \n",
    "        if self.add_alone:\n",
    "            df[\"Alone\"] = 0\n",
    "            df[\"Alone\"].loc[(df[\"SibSp\"]==0) & (df[\"Parch\"]==0)] = 1\n",
    "            \n",
    "            \n",
    "        if self.add_title: \n",
    "            df[\"Title\"] = df[\"Name\"].str.extract(r'((?<=, )[A-Za-z ]+(?=.))', expand = True)[0]\n",
    "            \n",
    "            \n",
    "            \n",
    "            df[\"Title\"] = df.loc[:,\"Title\"].replace(\n",
    "                ['Don', 'Rev', 'Dr', 'Mme', 'Major', \n",
    "                 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', \n",
    "                 'the Countess', 'Jonkheer', 'Dona'],\n",
    "                 'Rare')\n",
    "\n",
    "            df[\"Title\"] = df.loc[:,\"Title\"].replace(\n",
    "                ['Mrs','Miss','Ms'], 'Girls/Women')\n",
    "\n",
    "            df[\"Title\"] = df.loc[:,\"Title\"].replace(\n",
    "                ['Master'], 'Boys')\n",
    "\n",
    "            df[\"Title\"] = df.loc[:,\"Title\"].replace(\n",
    "                ['Mr'], 'Men') \n",
    "\n",
    "        \n",
    "        #Fix NaN values of Fare and Age by Title, Sex and Class:\n",
    "        fill_Nan = df.groupby([\"Title\", \"Sex\", \"Pclass\"])\n",
    "        \n",
    "        df[\"Age\"].loc[(df[\"Age\"] == 0)] = np.NaN\n",
    "        df[\"Age\"] = fill_Nan[\"Age\"].apply(lambda x: x.fillna(x.median()))\n",
    "                \n",
    "        \n",
    "        df[\"Fare\"].loc[(df[\"Fare\"] == 0)] = np.NaN\n",
    "        df[\"Fare\"] = fill_Nan[\"Fare\"].apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "        \n",
    "        if self.fare_per_fam:\n",
    "            df[\"fare_per_fam\"] = df[\"Fare\"]/df[\"Fam_size\"]\n",
    "            \n",
    "        if self.add_famid:\n",
    "            df[\"surname\"] = df[\"Name\"].str.split(\",\", n = 1, expand = True)[0]\n",
    "            \n",
    "            f_Id = lambda x: '-'.join(x.map(str))\n",
    "            df[\"Fam_ID\"] = df[[\"surname\", \"Pclass\", \"Embarked\", \"Cabin\"]\n",
    "                             ].apply(f_Id, axis=1)\n",
    "            \n",
    "            df[\"Fam_ID\"].loc[(df[\"Alone\"] == 1)] = 'Alone'\n",
    "\n",
    "\n",
    "        if add_ageG:\n",
    "            df[\"age_group\"] = pd.qcut(df.Age, q=4, labels=False)        \n",
    "    \n",
    "        if add_fareG:\n",
    "            df[\"Fare_group\"] = pd.qcut(df.Fare, q=6, labels=False) \n",
    "\n",
    "        \n",
    "        df.drop(labels=[\"Cabin\", \"Name\", \"Ticket\", \"surname\"], axis = 1, inplace = True)\n",
    "        \n",
    "        return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Features\n",
    "add_pipeline = Pipeline([\n",
    "    ('add_feat', add_features(list_add))\n",
    "])\n",
    "\n",
    "#Numeric Transformations\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', Selector(list_num)),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "#Catagorical Transformations\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', Selector(list_cat)),\n",
    "    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ('cat_encoder', OneHotEncoder(sparse=False)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = add_pipeline.fit_transform(titanic)\n",
    "\n",
    "#a = add_pipeline.fit_transform(X_tr)\n",
    "#b = add_pipeline.fit_transform(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Transformed Dataframe: {}'.format(list(a.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perpare_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline)\n",
    "])\n",
    "\n",
    "full_pipe = Pipeline([\n",
    "    (\"add_pipeline\", add_pipeline),\n",
    "    (\"prep_pipeline\", perpare_pipeline)\n",
    "])\n",
    "\n",
    "X_train = full_pipe.fit_transform(X_tr)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled Test Set for predictions:\n",
    "X_test = full_pipe.fit_transform(Test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create new model if pickle already even if pickle exists; load_pkl = False\n",
    "load_pkl = True\n",
    "\n",
    "cv_split=StratifiedKFold(n_splits=10)\n",
    "cv_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cross_val(clf, name_clf, lp=load_pickle):\n",
    "    if pickle_exist(str(name_clf)) and (lp==True):\n",
    "        ml_clf = load_pickle(str(name_clf))\n",
    "    else:\n",
    "        ml_clf = clf\n",
    "        ml_clf.fit(X_train, y_train)\n",
    "        save_pickle(str(name_clf), ml_clf)\n",
    "    cvs =  cross_val_score(ml_clf, X_train, y_train, cv=cv_split, scoring=\"accuracy\") \n",
    "    return  np.mean(cvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_options = [          \n",
    "    LinearSVC(), #no proba\n",
    "    SVC(probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    \n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(),\n",
    "    PassiveAggressiveClassifier(), #no proba\n",
    "    Perceptron(), #no proba\n",
    "    RidgeClassifier(), #no proba\n",
    "    \n",
    "    GaussianProcessClassifier(),\n",
    "    \n",
    "    BernoulliNB(),\n",
    "    GaussianNB(),\n",
    "    \n",
    "    DecisionTreeClassifier(),\n",
    "    \n",
    "    KNeighborsClassifier(),\n",
    "    \n",
    "    \n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(DecisionTreeClassifier(),\n",
    "                       learning_rate=0.1),\n",
    "    \n",
    "    ExtraTreesClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    \n",
    "    LinearDiscriminantAnalysis(),\n",
    "\n",
    "    MLPClassifier()\n",
    "\n",
    "] \n",
    " \n",
    "\n",
    "   \n",
    "compare_col = ['Clf Name', 'Clf Parameters', 'Clf Mean Accuracy'] \n",
    "clf_compare = pd.DataFrame(columns = compare_col)\n",
    "                 \n",
    "row=0\n",
    "          \n",
    "for clf in clf_options: \n",
    "    clf_name = clf.__class__.__name__\n",
    "                    \n",
    "    clf_compare.loc[row, 'Clf Name'] = clf_name\n",
    "    clf_compare.loc[row, 'Clf Parameters'] = str(clf.get_params()) \n",
    "    cvs = model_cross_val(clf, clf_name)\n",
    "    \n",
    "    clf_compare.loc[row, 'Clf Mean Accuracy'] = cvs\n",
    "    \n",
    "    row +=1\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_compare.sort_values(by = 'Clf Mean Accuracy', ascending = False, inplace = True)\n",
    "clf_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters of classifiers (Random Search):\n",
    "\n",
    "#Linear SVC\n",
    "lin_svc_rs = {\n",
    "    'loss' : ['hinge','squared_hinge'],\n",
    "    'C' : reciprocal(0.01, 1000)\n",
    "}\n",
    "\n",
    "\n",
    "#SVC\n",
    "svc_rs = {\n",
    "    'C' : reciprocal(0.01, 1000),\n",
    "    #'kernel' : ['linear', 'rbf'],\n",
    "    #'gamma' : reciprocal(0.01, 10000)\n",
    "}\n",
    "\n",
    "#NuSVC\n",
    "nuSVC_rs = {\n",
    "    'kernel' : ['linear', 'rbf'],\n",
    "    'gamma' : reciprocal(0.01, 10000),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "log_reg_rs = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'C' : reciprocal(0.01, 1000) \n",
    "}\n",
    "\n",
    "#SGDClassifier\n",
    "SGDC_rs = {\n",
    "    'loss' : ['hinge','squared_hinge', 'perceptron'],\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'alpha': reciprocal(0.1, 10000), \n",
    "    'warm_start' : [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PassiveAggressiveClassifier\n",
    "pass_rs = {\n",
    "    'C' : reciprocal(0.01, 1000),\n",
    "    'fit_intercept' : [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "#Perceptron\n",
    "perc_rs = {\n",
    "    'alpha': reciprocal(0.1, 10000), \n",
    "    'fit_intercept' : [True, False],\n",
    "    'shuffle' : [True, False],\n",
    "    'warm_start' : [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "#RidgeClassifier\n",
    "ridge_rs = {\n",
    "    'alpha': reciprocal(0.1, 10000)\n",
    "}\n",
    "\n",
    "#GaussianProcessClassifier\n",
    "gaus_rs = {\n",
    "    'warm_start' : [True, False]\n",
    "}\n",
    "\n",
    "    \n",
    "#BernoulliNB\n",
    "bernNB_rs = {\n",
    "    'alpha': reciprocal(0.1, 10000),\n",
    "    'binarize': reciprocal(0.1, 10000),\n",
    "    'fit_prior' : [True, False]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#GaussianNB\n",
    "guasNB_rs = {\n",
    "    'var_smoothing': reciprocal(1, 1000000),\n",
    "}\n",
    "\n",
    "#Decision Tree Classifier\n",
    "tree_rs = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'splitter' : ['best', 'random'],\n",
    "    'max_depth' : randint(1, 5),\n",
    "    'min_samples_split' : randint(2, 10),\n",
    "    'min_samples_leaf' : randint(1, 10),\n",
    "    'max_features' : randint(1, 10)\n",
    "}\n",
    "\n",
    "  \n",
    "#K-Neighbors Classifier\n",
    "k_neigh_rs = {\n",
    "    'n_neighbors': randint(3, 15),\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size' : randint(2, 100),\n",
    "    'p' : randint(1, 2)\n",
    "}\n",
    "\n",
    "#Random Forest Classifier\n",
    "forest_rs = {\n",
    "    'n_estimators' : randint(10, 500),\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth' : randint(1, 5),\n",
    "    'min_samples_split' : randint(2, 15),\n",
    "    'min_samples_leaf' : randint(1, 15),\n",
    "    'max_features' : randint(1, 15)\n",
    "}\n",
    "    \n",
    "#AdaBoost Classifier\n",
    "ada_rs = {\n",
    "    'base_estimator__criterion' : ['gini', 'entropy'],\n",
    "    'base_estimator__splitter' : ['best', 'random'],\n",
    "    'n_estimators' : randint(1, 50),\n",
    "    'learning_rate' : reciprocal(0.6, 10000),\n",
    "    'algorithm' : ['SAMME', 'SAMME.R']\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#Extra Trees Classifier\n",
    "extra_tree_rs = {\n",
    "    'n_estimators' : randint(10, 500),\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'min_samples_split' : randint(2, 10),\n",
    "    'min_samples_leaf' : randint(1, 10),\n",
    "    'max_features' : randint(1, 10)    \n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#Gradient Boosting Classifier\n",
    "grad_boost_rs = {\n",
    "    'loss' : ['deviance', 'exponential'],\n",
    "    'learning_rate' : reciprocal(0.6, 10000),\n",
    "    'n_estimators' : randint(10, 500),\n",
    "    'min_samples_split' : randint(2, 15),\n",
    "    'min_samples_leaf' : randint(1, 15),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'max_features' : randint(1, 15)\n",
    "    \n",
    "}  \n",
    "\n",
    "\n",
    "bag_rs = {\n",
    "    'n_estimators' : randint(10, 50)\n",
    "}\n",
    "\n",
    "#LinearDiscriminantAnalysis\n",
    "linDes_rs = {\n",
    "    'solver' : ['svd']\n",
    "}\n",
    "\n",
    "\n",
    "#MLPClassifier\n",
    "MLPC_rs = {\n",
    "    'hidden_layer_sizes' : randint(10, 1000),\n",
    "    'alpha': reciprocal(0.1, 10000),\n",
    "    'warm_start' : [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "param_option_rs = [\n",
    "    lin_svc_rs,\n",
    "    svc_rs,\n",
    "    nuSVC_rs,\n",
    "    log_reg_rs,\n",
    "    SGDC_rs,\n",
    "    pass_rs,\n",
    "    perc_rs,\n",
    "    ridge_rs,\n",
    "    gaus_rs,\n",
    "    bernNB_rs,\n",
    "    guasNB_rs,\n",
    "    tree_rs,\n",
    "    k_neigh_rs,\n",
    "    forest_rs,\n",
    "    ada_rs,\n",
    "    extra_tree_rs,\n",
    "    grad_boost_rs, \n",
    "    bag_rs,\n",
    "    linDes_rs,\n",
    "    MLPC_rs\n",
    "    \n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------\n",
    "def model_rand_gs(clf, name_clf, clf_param, lp=load_pickle): \n",
    "\n",
    "    rand_clf = RandomizedSearchCV(clf, param_distributions=clf_param, cv=cv_split, \n",
    "                          verbose=2, n_jobs=-1, n_iter=250, scoring='accuracy')\n",
    "    \n",
    "    rand_clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_est = rand_clf.best_estimator_\n",
    "    best_sco = rand_clf.best_score_ \n",
    "    \n",
    "    text_check = os.path.isfile(PICKLE_PATH+\"/best_score_rs_\"+str(name_clf)+\".txt\")\n",
    "    \n",
    "    if pickle_exist(\"best_rs_\"+str(name_clf)) and text_check and (lp==True):\n",
    "        #load current best score\n",
    "        prev_best_score = max(np.loadtxt(\n",
    "            (PICKLE_PATH+\"/best_score_rs_\"+str(name_clf)+\".txt\"), dtype=float))\n",
    "\n",
    "        if best_sco > prev_best_score:\n",
    "            temp = [best_sco, best_sco]\n",
    "            np.savetxt(\n",
    "                PICKLE_PATH+\"/best_score_rs_\"+str(name_clf)+\".txt\", temp, fmt='%f')\n",
    "            save_pickle(\"best_rs_\"+str(name_clf), best_est)\n",
    "        else:\n",
    "            #load in better parameters\n",
    "            best_sco = max(np.loadtxt(\n",
    "                PICKLE_PATH+\"/best_score_rs_\"+str(name_clf)+\".txt\", dtype=float))\n",
    "            best_est = load_pickle(\"best_rs_\"+str(name_clf))\n",
    "    else:\n",
    "        #make pickles if dont exist\n",
    "        temp = [best_sco, best_sco]\n",
    "        np.savetxt(\n",
    "            PICKLE_PATH+\"/best_score_rs_\"+str(name_clf)+\".txt\", temp, fmt='%f')\n",
    "        save_pickle(\"best_rs_\"+str(name_clf), best_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_col = ['Clf Name', 'Best Clf Parameters', 'Best Clf Accuracy Score'] \n",
    "best_compare_rs = pd.DataFrame(columns = best_col)\n",
    "          \n",
    "          \n",
    "row=0\n",
    "\n",
    "for clf, param in zip(clf_options, param_option_rs): \n",
    "    \n",
    "    clf_name = clf.__class__.__name__\n",
    "    best_compare_rs.loc[row, 'Clf Name'] = clf_name\n",
    "    \n",
    "    print(\"{}: {}\".format(row, clf_name))\n",
    "    \n",
    "    \n",
    "    #model_grid(clf, clf_name, param)\n",
    "    model_rand_gs(clf, clf_name, param)\n",
    "   \n",
    "    \n",
    "    best_score = max(np.loadtxt((PICKLE_PATH+\"/best_score_rs_\"+str(clf_name)+\".txt\"), dtype=float))\n",
    "    best_clf = load_pickle(\"best_rs_\"+str(clf_name))\n",
    "    \n",
    "    print(\"{}: {}\\n\".format(clf_name, best_clf.get_params))\n",
    "        \n",
    "    best_compare_rs.loc[row, 'Best Clf Parameters'] = str(best_clf.get_params())\n",
    "    best_compare_rs.loc[row, 'Best Clf Accuracy Score'] = str(best_score)\n",
    "    \n",
    "    row +=1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Random Grid-search\n",
    "best_compare_rs.sort_values(by = 'Best Clf Accuracy Score', ascending = False, inplace = True)\n",
    "\n",
    "notify(\"Python: Kaggle\", \"Random Search is complete\")\n",
    "best_compare_rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters of classifiers (Grid Search):\n",
    "\n",
    "#Linear SVC\n",
    "lin_svc_param = {\n",
    "    'loss' : ['hinge','squared_hinge'],\n",
    "    'C' : [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "#SVC\n",
    "svc_param = {\n",
    "    'C' : [1, 2, 5, 10],\n",
    "    'kernel' : ['linear', 'rbf'],\n",
    "    'gamma' : [ 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "#NuSVC\n",
    "nuSVC_param = {\n",
    "    'kernel' : ['linear', 'rbf'],\n",
    "    'gamma' : [ 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "#Logistic Regression\n",
    "log_reg_param = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'C': [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "#SGDClassifier\n",
    "SGDC_param = {\n",
    "    'loss' : ['hinge','squared_hinge', 'perceptron'],\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'alpha' : [0.001, 0.01, 0.1, 1],\n",
    "    'warm_start' : [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "#PassiveAggressiveClassifier\n",
    "pass_param = {\n",
    "    'C' : [1, 2, 5, 10],\n",
    "    'fit_intercept' : [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "#Perceptron\n",
    "perc_param = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1], \n",
    "    'fit_intercept' : [True, False],\n",
    "    'shuffle' : [True, False],\n",
    "    'warm_start' : [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "#RidgeClassifier\n",
    "ridge_param = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "#GaussianProcessClassifier\n",
    "gaus_param = {\n",
    "    'warm_start' : [True, False]\n",
    "}\n",
    "\n",
    "    \n",
    "#BernoulliNB\n",
    "bernNB_param = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1],\n",
    "    'binarize': [0.001, 0.01, 0.1, 1],\n",
    "    'fit_prior' : [True, False]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "#GaussianNB\n",
    "guasNB_param = {\n",
    "    'var_smoothing': [0.001, 0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "\n",
    "#Decision Tree Classifier\n",
    "tree_param = {\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'splitter' : ['best', 'random'],\n",
    "    'max_depth' : [1, 2, 3, 5],\n",
    "    'min_samples_split' : [2, 5, 10],\n",
    "    'min_samples_leaf' : [1, 3, 10],\n",
    "    'max_features' : [1, 5, 10]\n",
    "}\n",
    "\n",
    "#K-Neighbors Classifier\n",
    "k_neigh_param = {\n",
    "    'n_neighbors': [3, 4, 5, 10],\n",
    "    'weights' : ['uniform','distance'],\n",
    "    'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size' : [2, 10, 50, 100],\n",
    "    'p' : [1, 2]\n",
    "}\n",
    "\n",
    "\n",
    "#Random Forest Classifier\n",
    "forest_param = {\n",
    "    'n_estimators' : [10, 50, 100],\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'max_depth' : [1, 2, 5],\n",
    "    'min_samples_split' : [2, 5, 10],\n",
    "    'min_samples_leaf' : [1, 3, 10],\n",
    "    'max_features' : [1, 5, 10]\n",
    "}\n",
    "\n",
    "#AdaBoost Classifier\n",
    "ada_param = {\n",
    "    'base_estimator__criterion' : ['gini', 'entropy'],\n",
    "    'base_estimator__splitter' : ['best', 'random'],\n",
    "    'n_estimators' : [1, 2, 5, 10, 50],\n",
    "    'learning_rate' : [0.001, 0.01, 0.1, 0.5, 1],\n",
    "    'algorithm' : ['SAMME', 'SAMME.R']\n",
    "    \n",
    "}\n",
    "\n",
    "#Extra Trees Classifier\n",
    "extra_tree_param = {\n",
    "    'n_estimators' : [100, 200, 300, 500],\n",
    "    'criterion' : ['gini', 'entropy'],\n",
    "    'min_samples_split' : [2, 5, 10],\n",
    "    'min_samples_leaf' : [1, 5,10],\n",
    "    'max_features' : [1, 5, 10]    \n",
    "    \n",
    "}\n",
    "\n",
    "#Gradient Boosting Classifier\n",
    "grad_boost_param = {\n",
    "    'loss' : ['deviance', 'exponential'],\n",
    "    'learning_rate' : [0.01, 0.1, 1],\n",
    "    'n_estimators' : [100, 200],\n",
    "    'min_samples_split' : [2, 5, 10],\n",
    "    'min_samples_leaf' : [1, 10, 100],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'max_features': [0.1, 0.3]\n",
    "    \n",
    "}   \n",
    "\n",
    "#Bagging Classifier\n",
    "bag_param = {\n",
    "    'n_estimators' : [10, 20, 50, 100, 200, 500],\n",
    "}\n",
    "\n",
    "#LinearDiscriminantAnalysis\n",
    "linDes_param = {\n",
    "    'solver' : ['svd']\n",
    "}\n",
    "\n",
    "\n",
    "#MLPClassifier\n",
    "MLPC_param = {\n",
    "    'hidden_layer_sizes' : [10, 50, 100, 500, 1000],\n",
    "    'alpha': [0.001, 0.01, 0.1, 1],\n",
    "    'warm_start' : [True, False]\n",
    "}\n",
    "               \n",
    "                \n",
    "param_option_gs = [\n",
    "    lin_svc_param, #-\n",
    "    svc_param, #-\n",
    "    nuSVC_param, #-\n",
    "    log_reg_param, #-\n",
    "    SGDC_param, #-\n",
    "    pass_param, #-\n",
    "    perc_param, #-\n",
    "    ridge_param, #-\n",
    "    gaus_param,\n",
    "    bernNB_param,\n",
    "    guasNB_param,\n",
    "    tree_param,\n",
    "    k_neigh_param,\n",
    "    forest_param,\n",
    "    ada_param,\n",
    "    extra_tree_param,\n",
    "    grad_boost_param,\n",
    "    bag_param,\n",
    "    linDes_param,\n",
    "    MLPC_param\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_grid(clf, name_clf, clf_param, lp=load_pickle): \n",
    "\n",
    "    gs_clf = GridSearchCV(clf, param_grid=clf_param, cv=cv_split, \n",
    "                          verbose=2, n_jobs=-1, scoring='accuracy')\n",
    "    \n",
    "    gs_clf.fit(X_train, y_train)\n",
    "    \n",
    "    best_est = gs_clf.best_estimator_\n",
    "    best_sco = gs_clf.best_score_ \n",
    "    \n",
    "    text_check = os.path.isfile(PICKLE_PATH+\"/best_score_gs_\"+str(name_clf)+\".txt\")\n",
    "    \n",
    "    if pickle_exist(\"best_gs_\"+str(name_clf)) and text_check and (lp==True):\n",
    "        #load current best score\n",
    "        prev_best_score = max(np.loadtxt((PICKLE_PATH+\"/best_score_gs_\"+str(name_clf)+\".txt\"), dtype=float))\n",
    "\n",
    "        if best_sco > prev_best_score:\n",
    "            temp = [best_sco, best_sco]\n",
    "            np.savetxt(PICKLE_PATH+\"/best_score_gs_\"+str(name_clf)+\".txt\", temp, fmt='%f')\n",
    "            save_pickle(\"best_gs_\"+str(name_clf), best_est)\n",
    "        else:\n",
    "            #load in better parameters\n",
    "            best_sco = max(np.loadtxt(PICKLE_PATH+\"/best_score_gs_\"+str(name_clf)+\".txt\", dtype=float))\n",
    "            best_est = load_pickle(\"best_gs_\"+str(name_clf))\n",
    "    else:\n",
    "        #make pickles if dont exist\n",
    "        temp = [best_sco, best_sco]\n",
    "        np.savetxt(PICKLE_PATH+\"/best_score_gs_\"+str(name_clf)+\".txt\", temp, fmt='%f')\n",
    "        save_pickle(\"best_gs_\"+str(name_clf), best_est)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_col = ['Clf Name', 'Best Clf Parameters', 'Best Clf Accuracy Score'] \n",
    "best_compare_gs = pd.DataFrame(columns = best_col)\n",
    "          \n",
    "          \n",
    "row=0\n",
    "\n",
    "for clf, param in zip(clf_options, param_option_gs): \n",
    "\n",
    "    \n",
    "    clf_name = clf.__class__.__name__\n",
    "    best_compare_gs.loc[row, 'Clf Name'] = clf_name\n",
    "    \n",
    "    print(\"{}: {}\".format(row, clf_name))\n",
    "    \n",
    "    model_grid(clf, clf_name, param)\n",
    "    \n",
    "    best_score = max(np.loadtxt((PICKLE_PATH+\"/best_score_gs_\"+str(clf_name)+\".txt\"), dtype=float))\n",
    "    best_clf = load_pickle(\"best_gs_\"+str(clf_name))\n",
    "    \n",
    "    best_compare_gs.loc[row, 'Best Clf Parameters'] = str(best_clf.get_params())\n",
    "    best_compare_gs.loc[row, 'Best Clf Accuracy Score'] = str(best_score)\n",
    "    \n",
    "    row +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After Gridseach\n",
    "best_compare_gs.sort_values(by = 'Best Clf Accuracy Score', ascending = False, inplace = True)\n",
    "\n",
    "notify(\"Python: Kaggle\", \"Grid Search is complete\")\n",
    "best_compare_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_clf(clf):\n",
    "    clf_name = clf.__class__.__name__\n",
    "    \n",
    "    gs_score = max(np.loadtxt((PICKLE_PATH+\"/best_score_gs_\"+str(clf_name)+\".txt\"), dtype=float))\n",
    "    rs_score = max(np.loadtxt((PICKLE_PATH+\"/best_score_rs_\"+str(clf_name)+\".txt\"), dtype=float))\n",
    "    \n",
    "    if gs_score > rs_score:\n",
    "        return load_pickle(\"best_gs_\"+str(clf_name))\n",
    "    else:\n",
    "        return load_pickle(\"best_rs_\"+str(clf_name))\n",
    "\n",
    "    \n",
    "    \n",
    "def load_best_score(clf):\n",
    "    clf_name = clf.__class__.__name__\n",
    "    \n",
    "    gs_score = max(np.loadtxt((PICKLE_PATH+\"/best_score_gs_\"+str(clf_name)+\".txt\"), dtype=float))\n",
    "    rs_score = max(np.loadtxt((PICKLE_PATH+\"/best_score_rs_\"+str(clf_name)+\".txt\"), dtype=float))\n",
    "    \n",
    "    if gs_score > rs_score:\n",
    "        return gs_score\n",
    "    else:\n",
    "        return rs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_col = ['Clf Name', 'Best Clf Parameters', 'Best Clf Accuracy Score'] \n",
    "best_rs_or_gs = pd.DataFrame(columns = best_col)\n",
    "row=0\n",
    "\n",
    "for clf in clf_options:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    best_rs_or_gs.loc[row, 'Clf Name'] = clf_name\n",
    "    \n",
    "    best_clf  = load_best_clf(clf)\n",
    "    best_score = load_best_score(clf)\n",
    "    \n",
    "    best_rs_or_gs.loc[row, 'Best Clf Parameters'] = str(best_clf.get_params())\n",
    "    best_rs_or_gs.loc[row, 'Best Clf Accuracy Score'] = str(best_score)\n",
    "    \n",
    "    row +=1\n",
    "    \n",
    "best_rs_or_gs.sort_values(by = 'Best Clf Accuracy Score', ascending = False, inplace = True)\n",
    "best_rs_or_gs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance:\n",
    "\n",
    "#NOTE: add val set and retrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, ver_index in split.split(Train, Train[\"Survived\"]):\n",
    "    Train_strat = Train.loc[train_index]\n",
    "    Ver_strat = Train.loc[ver_index]\n",
    "\n",
    "print(\"Training set: {} entries, Verificaiton set: {} entries\".format(len(Train_strat),len(Ver_strat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_training = Train_strat[\"Survived\"].copy()\n",
    "X_tr = Train_strat.drop(\"Survived\", axis=1)\n",
    "X_training = full_pipe.fit_transform(X_tr)\n",
    "\n",
    "y_verification = Ver_strat[\"Survived\"].copy()\n",
    "X_ver = Ver_strat.drop(\"Survived\", axis=1)\n",
    "X_verification = full_pipe.fit_transform(X_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardVote = True\n",
    "voter = 'soft'\n",
    "\n",
    "\n",
    "\n",
    "#only probabilities: (ie default soft voting)\n",
    "svc = load_best_clf(clf_options[1])\n",
    "nu_svc = load_best_clf(clf_options[2])\n",
    "log_reg = load_best_clf(clf_options[3])\n",
    "gaus= load_best_clf(clf_options[8])\n",
    "bernNB = load_best_clf(clf_options[9])\n",
    "guasNB = load_best_clf(clf_options[10])\n",
    "tree = load_best_clf(clf_options[11])\n",
    "knn = load_best_clf(clf_options[12])\n",
    "forest = load_best_clf(clf_options[13])\n",
    "ada = load_best_clf(clf_options[14])\n",
    "etree = load_best_clf(clf_options[15])\n",
    "gradb = load_best_clf(clf_options[16])\n",
    "bag = load_best_clf(clf_options[17])\n",
    "linDes = load_best_clf(clf_options[18])\n",
    "MLPC = load_best_clf(clf_options[19])\n",
    "\n",
    "\n",
    "est = [('svc', svc), ('nu_svc', nu_svc), ('log_reg', log_reg),\n",
    "       ('gaus', gaus), ('bernNB', bernNB), ('guasNB', guasNB), \n",
    "       ('tree', tree), ('knn', knn), ('forest', forest), \n",
    "       ('ada', ada), ('etree', etree), ('gradb', gradb), \n",
    "       ('bag', bag), ('linDes', linDes), ('MLPC', MLPC)]\n",
    "\n",
    "if hardVote:\n",
    "    #hard Voting: add probabilities\n",
    "    voter = 'hard'\n",
    "    \n",
    "    lin_svc = load_best_clf(clf_options[0])\n",
    "    SGDC = load_best_clf(clf_options[4])\n",
    "    passC = load_best_clf(clf_options[5])\n",
    "    perc = load_best_clf(clf_options[6])\n",
    "    ridge = load_best_clf(clf_options[7])\n",
    "    \n",
    "    est_add = [('lin_svc', lin_svc), ('SGDC', SGDC),\n",
    "               ('passC', passC), ('perc', perc), ('ridge', ridge)]\n",
    "    \n",
    "    est.extend(est_add)\n",
    "    \n",
    "    \n",
    "\n",
    "print(est)\n",
    "vote_clf = VotingClassifier(estimators=est, voting=voter)\n",
    "\n",
    "vote_clf.fit(X_training, y_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in (lin_svc, svc, log_reg, forest, \n",
    "            SGDC, tree, ada, etree, knn, \n",
    "            gradb, vote_clf):#, XGBC, vote_clf):\n",
    "\n",
    "    clf.fit(X_training, y_training)\n",
    "    y_pred = clf.predict(X_verification)\n",
    "    \n",
    "    name = clf.__class__.__name__\n",
    "    score = accuracy_score(y_verification, y_pred)\n",
    "    \n",
    "    print(\"{}: {:.2f}%\".format(name, 100*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit to whole dataset\n",
    "vote_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hardVote:\n",
    "    vote_list = (svc, nu_svc, log_reg, gaus, bernNB, \n",
    "                 guasNB, tree, knn, forest, ada, \n",
    "                 etree, gradb, bag, linDes, MLPC)\n",
    "        \n",
    "else:\n",
    "    vote_list = (svc, nu_svc, log_reg, SGDC, gaus, \n",
    "                 bernNB, guasNB, tree, knn, forest, \n",
    "                 ada, etree, gradb, bag, linDes, MLPC, \n",
    "                 lin_svc, passC, perc, ridge)\n",
    "        \n",
    "\n",
    "for clf in vote_list:\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_train)\n",
    "    \n",
    "    name = clf.__class__.__name__\n",
    "    score = accuracy_score(y_train, y_pred)\n",
    "    \n",
    "    print(\"{}: {:.2f}%\".format(name, 100*score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(csv_name, save_loc=TITANIC_PATH):\n",
    "    curr_path = os.getcwd()\n",
    "    save_path = os.path.join(curr_path, save_loc)\n",
    "    os.chdir(save_path)\n",
    "    \n",
    "    max_i = 0\n",
    "    \n",
    "    len_name = len(csv_name)\n",
    "           \n",
    "    for file in glob.glob(csv_name+'*.csv'):\n",
    "        \n",
    "        file_name = file[:len(file)-4]\n",
    "        file_ver = file_name[len_name:]\n",
    "        \n",
    "        if int(file_ver) > max_i:\n",
    "            max_i = int(file_ver)\n",
    "        \n",
    "    new_ver = csv_name+str(max_i+1)+'.csv'\n",
    "        \n",
    "        \n",
    "        \n",
    "    os.chdir(curr_path)\n",
    "    \n",
    "    return os.path.join(save_path, new_ver)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PassengerId = Test['PassengerId']\n",
    "\n",
    "Survived_pred = vote_clf.predict(X_test) \n",
    "\n",
    "\n",
    "Submission = pd.DataFrame({ 'PassengerId': PassengerId,\n",
    "                            'Survived': Survived_pred })\n",
    "\n",
    "name = \"Submission\"\n",
    "file_name = make_csv(name)\n",
    "\n",
    "Submission.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 5 clf:\n",
    "best_idx = best_rs_or_gs[\"Clf Name\"][0:5].index\n",
    "\n",
    "\n",
    "clf1 = load_best_clf(clf_options[best_idx[0]])\n",
    "clf2 = load_best_clf(clf_options[best_idx[1]])\n",
    "clf3 = load_best_clf(clf_options[best_idx[2]])\n",
    "clf4 = load_best_clf(clf_options[best_idx[3]])\n",
    "clf5 = load_best_clf(clf_options[best_idx[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_best = [(clf1.__class__.__name__, clf1), \n",
    "            (clf2.__class__.__name__, clf2),\n",
    "            (clf3.__class__.__name__, clf3),\n",
    "            (clf4.__class__.__name__, clf4),\n",
    "            (clf5.__class__.__name__, clf5)]\n",
    "    \n",
    "    \n",
    "Best_vote = VotingClassifier(estimators=est_best, voting=voter)\n",
    "\n",
    "Best_vote.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Predicitons:\n",
    "PassengerId = Test['PassengerId']\n",
    "\n",
    "Survived_pred = Best_vote.predict(X_test) \n",
    "\n",
    "\n",
    "Submission = pd.DataFrame({ 'PassengerId': PassengerId,\n",
    "                            'Survived': Survived_pred })\n",
    "\n",
    "name = \"B_Submission\"\n",
    "file_name = make_csv(name)\n",
    "\n",
    "Submission.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
